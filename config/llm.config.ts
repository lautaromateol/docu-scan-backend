export const LLM_CONFIG = {
  model: 'meta-llama/llama-4-scout-17b-16e-instruct',
  temperature: 0.2,
  max_completion_tokens: 2048,
  top_p: 1,
  stop: null,
  stream: false, 
}
